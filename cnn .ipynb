{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.transforms.v2 as v2 \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim \n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306a81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb888e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1999f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/Users/prasonagnihottri/Downloads/Alzheimer_s Dataset'\n",
    "test_ds = os.path.join(dir_path, 'test')\n",
    "train_ds = os.path.join(dir_path, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb3ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageFolder(train_ds)\n",
    "test = ImageFolder(test_ds)\n",
    "\n",
    "class_map = {0:'Mild Demented', 1:'Moderate Demented', 2:'Non Demented', 3:'Very Mild Demented'}\n",
    "class_dict = dict(Counter(train.targets))\n",
    "class_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7654eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for path in train.imgs:\n",
    "    image_paths.append(path[0])\n",
    "shuffled = image_paths.copy()    \n",
    "random.shuffle(shuffled)\n",
    "plt.figure(figsize=(5,5))\n",
    "for i, path in enumerate(shuffled):\n",
    "    if i>3: break\n",
    "    plt.subplot(2,2,i+1)\n",
    "    img = Image.open(path)\n",
    "    image_class = Path(path).parent.stem\n",
    "    plt.title(image_class)\n",
    "    plt.imshow(img)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493256d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLoader(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.dataset[idx][0])\n",
    "        class_category = self.dataset[idx][1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, class_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c780623",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.functional.get_image_size(Image.open(train.imgs[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c926187",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.transforms.functional.get_image_num_channels(Image.open(train.imgs[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf99791",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = v2.Compose([\n",
    "                v2.Resize((128,128)),\n",
    "                v2.Grayscale(num_output_channels=1),\n",
    "                #v2.RandomHorizontalFlip(p=0.5), # randomly flip and rotate\n",
    "                #v2.ColorJitter(0.3,0.4,0.4,0.2),\n",
    "                #v2.RandomCrop(size=(128,128)),\n",
    "                v2.ToTensor(),\n",
    "                #v2.Normalize((0.425, 0.415, 0.405), (0.205, 0.205, 0.205))\n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "                v2.Resize((128,128)),\n",
    "                v2.Grayscale(num_output_channels=1),\n",
    "                v2.ToTensor(),\n",
    "                #v2.Normalize((0.425, 0.415, 0.405), (0.205, 0.205, 0.205))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83454e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label = train.imgs, train.targets\n",
    "test_data, test_label = test.imgs, test.targets\n",
    "\n",
    "train_dataset = ImageLoader(train_data, train_transform)\n",
    "test_dataset = ImageLoader(test_data, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_data, val_data= torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce770b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64)\n",
    "val_loader = DataLoader(val_data, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.transforms.functional.get_image_num_channels(next(iter(train_dataset))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=4096, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x))) \n",
    "        x = F.relu(self.pool(self.conv2(x)))  \n",
    "        x = F.relu(self.pool(self.conv3(x)))\n",
    "        x = F.relu(self.pool(self.conv4(x)))  \n",
    "        \n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = Net(num_classes=4).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec834ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[6,10,15], gamma=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6984bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca4b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "num_epochs=20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels.data)\n",
    "            all_preds.extend(preds.view(-1).cpu().numpy())\n",
    "            all_labels.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_accuracy = val_corrects.double() / len(val_loader.dataset)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title(\"Training and Validation Losses per Epoch\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f15563",
   "metadata": {},
   "outputs": [],
   "source": [
    " acc=[]\n",
    "for i in val_accuracies:\n",
    "    acc.append(i.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0427b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(acc, label='Validation Accuracy')\n",
    "plt.title(\"Validation Accuracy per Epoch\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ad4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_class(img):\n",
    "    img_tens = test_transform(img)\n",
    "    img_im = img_tens.unsqueeze(0).to(device)\n",
    "    uinput = Variable(img_im)\n",
    "    uinput = uinput.to(device)\n",
    "    out = model(uinput)\n",
    "\n",
    "    index = out.data.cpu().numpy().argmax()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = os.listdir('/Users/prasonagnihottri/Downloads/Alzheimer_s Dataset')\n",
    "classes = {k:v for k , v in enumerate(sorted(outcomes))}\n",
    "classes\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58243a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_paths = []\n",
    "for path in test.imgs:\n",
    "    test_image_paths.append(path[0])\n",
    "shuffled = test_image_paths.copy()    \n",
    "random.shuffle(shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceedb5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for i, images in enumerate(shuffled):\n",
    "    # just want 25 images to print\n",
    "    if i > 24:break\n",
    "    img = Image.open(images)\n",
    "    index = pred_class(img)\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.title(classes[index])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e32224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e3fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c082116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
